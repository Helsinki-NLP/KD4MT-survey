@inproceedings{bucila2006model,
  title={Model compression},
  author={Buciluǎ, Cristian and Caruana, Rich and Niculescu-Mizil, Alexandru},
  booktitle={Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining},
  pages={535--541},
  year={2006}
}


@inproceedings{kim2016sequence,
  title={Sequence-Level Knowledge Distillation},
  author={Kim, Yoon and Rush, Alexander M},
  booktitle={Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing},
  pages={1317--1327},
  year={2016}
}

@book{koehn-2020-neural,
    author = {Philp Koehn},
    title = {Neural Machine Translation},
    publisher = {Cambridge University Press},
    year = 2020,
}

@article{freitag2017ensemble,
  title={Ensemble distillation for neural machine translation},
  author={Freitag, Markus and Al-Onaizan, Yaser and Sankaran, Baskaran},
  journal={arXiv preprint arXiv:1702.01802},
  year={2017}
}

@article{tan2019multilingual,
  title={Multilingual neural machine translation with knowledge distillation},
  author={Tan, Xu and Ren, Yi and He, Di and Qin, Tao and Zhao, Zhou and Liu, Tie-Yan},
  journal={arXiv preprint arXiv:1902.10461},
  year={2019}
}

@article{gou2021knowledge,
  title={Knowledge distillation: A survey},
  author={Gou, Jianping and Yu, Baosheng and Maybank, Stephen J and Tao, Dacheng},
  journal={International Journal of Computer Vision},
  volume={129},
  pages={1789--1819},
  year={2021},
  publisher={Springer}
}

@inproceedings{jooste2022knowledge,
  title={Knowledge Distillation for Sustainable Neural Machine Translation},
  author={Jooste, Wandri and Way, Andy and Haque, Rejwanul and Superbo, Riccardo},
  booktitle={Proceedings of the 15th Biennial Conference of the Association for Machine Translation in the Americas (Volume 2: Users and Providers Track and Government Track)},
  pages={221--230},
  year={2022}
}

@article{mghabbar2020building,
  title={Building a multi-domain neural machine translation model using knowledge distillation},
  author={Mghabbar, Idriss and Ratnamogan, Pirashanth},
  journal={arXiv preprint arXiv:2004.07324},
  year={2020}
}


@article{zhang2020improving,
  title={Improving low-resource neural machine translation with teacher-free knowledge distillation},
  author={Zhang, Xinlu and Li, Xiao and Yang, Yating and Dong, Rui},
  journal={IEEE Access},
  volume={8},
  pages={206638--206645},
  year={2020},
  publisher={IEEE}
}

@inproceedings{chen2017teacher,
  title={A Teacher-Student Framework for Zero-Resource Neural Machine Translation},
  author={Chen, Yun and Liu, Yang and Cheng, Yong and Li, Victor OK},
  booktitle={Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={1925--1935},
  year={2017}
}

@misc{see2016compression,
      title={Compression of Neural Machine Translation Models via Pruning}, 
      author={Abigail See and Minh-Thang Luong and Christopher D. Manning},
      year={2016},
      eprint={1606.09274},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@article{sun2020knowledge,
  title={Knowledge distillation for multilingual unsupervised neural machine translation},
  author={Sun, Haipeng and Wang, Rui and Chen, Kehai and Utiyama, Masao and Sumita, Eiichiro and Zhao, Tiejun},
  journal={arXiv preprint arXiv:2004.10171},
  year={2020}
}

@article{hahn2019self,
  title={Self-knowledge distillation in natural language processing},
  author={Hahn, Sangchul and Choi, Heeyoul},
  journal={arXiv preprint arXiv:1908.01851},
  year={2019}
}

@inproceedings{saleh2020collective,
    title = "Collective Wisdom: Improving Low-resource Neural Machine Translation using Adaptive Knowledge Distillation",
    author = "Saleh, Fahimeh  and
      Buntine, Wray  and
      Haffari, Gholamreza",
    editor = "Scott, Donia  and
      Bel, Nuria  and
      Zong, Chengqing",
    booktitle = "Proceedings of the 28th International Conference on Computational Linguistics",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2020.coling-main.302",
    doi = "10.18653/v1/2020.coling-main.302",
    pages = "3413--3421",
    abstract = "Scarcity of parallel sentence-pairs poses a significant hurdle for training high-quality Neural Machine Translation (NMT) models in bilingually low-resource scenarios. A standard approach is transfer learning, which involves taking a model trained on a high-resource language-pair and fine-tuning it on the data of the low-resource MT condition of interest. However, it is not clear generally which high-resource language-pair offers the best transfer learning for the target MT setting. Furthermore, different transferred models may have complementary semantic and/or syntactic strengths, hence using only one model may be sub-optimal. In this paper, we tackle this problem using knowledge distillation, where we propose to distill the knowledge of ensemble of teacher models to a single student model. As the quality of these teacher models varies, we propose an effective adaptive knowledge distillation approach to dynamically adjust the contribution of the teacher models during the distillation process. Experiments on transferring from a collection of six language pairs from IWSLT to five low-resource language-pairs from TED Talks demonstrate the effectiveness of our approach, achieving up to +0.9 BLEU score improvements compared to strong baselines.",
}

@misc{hinton2015distilling,
      title={Distilling the Knowledge in a Neural Network}, 
      author={Geoffrey Hinton and Oriol Vinyals and Jeff Dean},
      year={2015},
      eprint={1503.02531},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@inproceedings{germann2020speed,
  title={Speed-optimized, compact student models that distill knowledge from a larger teacher model: the uedin-cuni submission to the wmt 2020 news translation task},
  author={Germann, Ulrich and Grundkiewicz, Roman and Popel, Martin and Dobreva, Radina and Bogoychev, Nikolay and Heafield, Kenneth},
  booktitle={Proceedings of the Fifth Conference on Machine Translation},
  pages={191--196},
  year={2020}
}

@article{dabre2020survey,
  title={A survey of multilingual neural machine translation},
  author={Dabre, Raj and Chu, Chenhui and Kunchukuttan, Anoop},
  journal={ACM Computing Surveys (CSUR)},
  volume={53},
  number={5},
  pages={1--38},
  year={2020},
  publisher={ACM New York, NY, USA}
}

@article{costa2022nllb,
  title={No language left behind: Scaling human-centered machine translation},
  author={Costa-juss{\`a}, Marta R and Cross, James and {\c{C}}elebi, Onur and Elbayad, Maha and Heafield, Kenneth and Heffernan, Kevin and Kalbassi, Elahe and Lam, Janice and Licht, Daniel and Maillard, Jean and others},
  journal={arXiv preprint arXiv:2207.04672},
  year={2022}
}

@inproceedings{gumma2023empirical,
  title={An Empirical Study of Leveraging Knowledge Distillation for Compressing Multilingual Neural Machine Translation Models},
  author={Gumma, Varun and Dabre, Raj and Kumar, Pratyush},
  booktitle={Proceedings of the 24th Annual Conference of the European Association for Machine Translation},
  pages={103--114},
  year={2023}
}


@inproceedings{galiano2023exploiting,
  title={Exploiting large pre-trained models for low-resource neural machine translation},
  author={Galiano-Jim{\'e}nez, Aar{\'o}n and S{\'a}nchez-Mart{\'\i}nez, Felipe and S{\'a}nchez-Cartagena, V{\'\i}ctor M and P{\'e}rez-Ortiz, Juan Antonio},
  booktitle={Proceedings of the 24th Annual Conference of the European Association for Machine Translation},
  pages={59--68},
  year={2023}
}


@inproceedings{tang2021multilingual,
  title={Multilingual translation from denoising pre-training},
  author={Tang, Yuqing and Tran, Chau and Li, Xian and Chen, Peng-Jen and Goyal, Naman and Chaudhary, Vishrav and Gu, Jiatao and Fan, Angela},
  booktitle={Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021},
  pages={3450--3466},
  year={2021}
}

@inproceedings{de-gibert-etal-2023-four,
    title = "Four Approaches to Low-Resource Multilingual {NMT}: The {H}elsinki Submission to the {A}mericas{NLP} 2023 Shared Task",
    author = {De Gibert, Ona  and
      V{\'a}zquez, Ra{\'u}l  and
      Aulamo, Mikko  and
      Scherrer, Yves  and
      Virpioja, Sami  and
      Tiedemann, J{\"o}rg},
    editor = "Mager, Manuel  and
      Ebrahimi, Abteen  and
      Oncevay, Arturo  and
      Rice, Enora  and
      Rijhwani, Shruti  and
      Palmer, Alexis  and
      Kann, Katharina",
    booktitle = "Proceedings of the Workshop on Natural Language Processing for Indigenous Languages of the Americas (AmericasNLP)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.americasnlp-1.20",
    doi = "10.18653/v1/2023.americasnlp-1.20",
    pages = "177--191",
    abstract = "The Helsinki-NLP team participated in the AmericasNLP 2023 Shared Task with 6 submissions for all 11 language pairs arising from 4 different multilingual systems. We provide a detailed look at the work that went into collecting and preprocessing the data that led to our submissions. We explore various setups for multilingual Neural Machine Translation (NMT), namely knowledge distillation and transfer learning, multilingual NMT including a high-resource language (English), language-specific fine-tuning, and multilingual NMT exclusively using low-resource data. Our multilingual Model B ranks first in 4 out of the 11 language pairs.",
}

@inproceedings{ebrahimi-etal-2023-findings,
    title = "Findings of the {A}mericas{NLP} 2023 Shared Task on Machine Translation into Indigenous Languages",
    author = "Ebrahimi, Abteen  and
      Mager, Manuel  and
      Rijhwani, Shruti  and
      Rice, Enora  and
      Oncevay, Arturo  and
      Baltazar, Claudia  and
      Cort{\'e}s, Mar{\'\i}a  and
      Monta{\~n}o, Cynthia  and
      Ortega, John E.  and
      Coto-solano, Rolando  and
      Cruz, Hilaria  and
      Palmer, Alexis  and
      Kann, Katharina",
    editor = "Mager, Manuel  and
      Ebrahimi, Abteen  and
      Oncevay, Arturo  and
      Rice, Enora  and
      Rijhwani, Shruti  and
      Palmer, Alexis  and
      Kann, Katharina",
    booktitle = "Proceedings of the Workshop on Natural Language Processing for Indigenous Languages of the Americas (AmericasNLP)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.americasnlp-1.23",
    doi = "10.18653/v1/2023.americasnlp-1.23",
    pages = "206--219",
    abstract = "In this work, we present the results of the AmericasNLP 2023 Shared Task on Machine Translation into Indigenous Languages of the Americas. This edition of the shared task featured eleven language pairs, one of which {--} Chatino-Spanish {--} uses a newly collected evaluation dataset, consisting of professionally translated text from the legal domain. Seven teams participated in the shared task, with a total of 181 submissions. Additionally, we conduct a human evaluation of the best system outputs, and compare them to the best submissions from the prior shared task. We find that this analysis agrees with the quantitative measures used to rank submissions, which shows further improvements of 9.64 ChrF on average across all languages, when compared to the prior winning system.",
}

@inproceedings{sheffield2023AmericasNLP,
    title = "{S}heffield{'}s Submission to the {A}mericas{NLP} Shared Task on Machine Translation into Indigenous Languages",
    author = "Gow-Smith, Edward  and
      S{\'a}nchez Villegas, Danae",
    editor = "Mager, Manuel  and
      Ebrahimi, Abteen  and
      Oncevay, Arturo  and
      Rice, Enora  and
      Rijhwani, Shruti  and
      Palmer, Alexis  and
      Kann, Katharina",
    booktitle = "Proceedings of the Workshop on Natural Language Processing for Indigenous Languages of the Americas (AmericasNLP)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.americasnlp-1.21",
    doi = "10.18653/v1/2023.americasnlp-1.21",
    pages = "192--199",
    abstract = "The University of Sheffield took part in the shared task 2023 AmericasNLP for all eleven language pairs. Our models consist of training different variations of NLLB-200 model on data provided by the organizers and available data from various sources such as constitutions, handbooks and news articles. Our models outperform the baseline model on the development set on chrF with substantial improvements particularly for Aymara, Guarani and Quechua. On the test set, our best submission achieves the highest average chrF of all the submissions, we rank first in four of the eleven languages, and at least one of our models ranks in the top 3 for all languages.",
}

@inproceedings{huang2023towards,
  title={Towards Higher Pareto Frontier in Multilingual Machine Translation},
  author={Feng, Xiaocheng and Geng, Xinwei and Li, Baohang and Qin, Bing and others},
  booktitle={The 61st Annual Meeting Of The Association For Computational Linguistics},
  year={2023}
}


@inproceedings{huang-etal-2022-unifying,
    title = "Unifying the Convergences in Multilingual Neural Machine Translation",
    author = "Huang, Yichong  and
      Feng, Xiaocheng  and
      Geng, Xinwei  and
      Qin, Bing",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.458",
    doi = "10.18653/v1/2022.emnlp-main.458",
    pages = "6822--6835",
    abstract = "Although all-in-one-model multilingual neural machine translation (MNMT) has achieved remarkable progress, the convergence inconsistency in the joint training is ignored, i.e., different language pairs reaching convergence in different epochs. This leads to the trained MNMT model over-fitting low-resource language translations while under-fitting high-resource ones. In this paper, we propose a novel training strategy named LSSD (LanguageSpecific Self-Distillation), which can alleviate the convergence inconsistency and help MNMT models achieve the best performance on each language pair simultaneously. Specifically, LSSD picks up language-specific best checkpoints for each language pair to teach the current model on the fly. Furthermore, we systematically explore three sample-level manipulations of knowledge transferring. Experimental results on three datasets show that LSSD obtains consistent improvements towards all language pairs and achieves the state-of-the-art.",
}

@article{zhao2022life,
  title={Life-long Learning for Multilingual Neural Machine Translation with Knowledge Distillation},
  author={Zhao, Yang and Zhu, Junnan and Xiang, Lu and Zhang, Jiajun and Zhou, Yu and Zhai, Feifei and Zong, Chengqing},
  journal={arXiv preprint arXiv:2212.02800},
  year={2022}
}

@inproceedings{stap-2023-multilingual,
    title = "Multilingual $k$-Nearest-Neighbor Machine Translation",
    author = "Stap, David  and
      Monz, Christof",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.571",
    doi = "10.18653/v1/2023.emnlp-main.571",
    pages = "9200--9208",
    abstract = "\textit{k}-nearest-neighbor machine translation has demonstrated remarkable improvements in machine translation quality by creating a datastore of cached examples. However, these improvements have been limited to high-resource language pairs, with large datastores, and remain a challenge for low-resource languages. In this paper, we address this issue by combining representations from multiple languages into a single datastore. Our results consistently demonstrate substantial improvements not only in low-resource translation quality (up to $+3.6$ BLEU), but also for high-resource translation quality (up to $+0.5$ BLEU). Our experiments show that it is possible to create multilingual datastores that are a quarter of the size, achieving a 5.3x speed improvement, by using linguistic similarities for datastore creation.",
}


@article{doo-dee-2023-target,
author = {Do, Heejin and Lee, Gary Geunbae},
title = {Target-Oriented Knowledge Distillation with Language-Family-Based Grouping for Multilingual NMT},
year = {2023},
issue_date = {February 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {2375-4699},
url = {https://doi.org/10.1145/3546067},
doi = {10.1145/3546067},
abstract = {Multilingual NMT has developed rapidly, but still has performance degradation caused by language diversity and model capacity constraints. To achieve the competitive accuracy of multilingual translation despite such limitations, knowledge distillation, which improves the student network by matching the teacher network’s output, has been applied and shown enhancement by focusing on the important parts of the teacher distribution. However, existing knowledge distillation methods for multilingual NMT rarely consider the knowledge, which has an important function as the student model’s target, in the process. In this article, we propose two distillation strategies that effectively use the knowledge to improve the accuracy of multilingual NMT. First, we introduce a language-family-based approach, guiding to select appropriate knowledge for each language pair. By distilling the knowledge of multilingual teachers that each processes a group of languages classified by language families, the multilingual model overcomes accuracy degradation caused by linguistic diversity. Second, we propose target-oriented knowledge distillation, which intensively focuses on the ground-truth target of knowledge with a penalty strategy. Our method provides a sensible distillation by penalizing samples without actual targets, while additionally targeting the ground-truth targets. Experiments using TED Talk datasets demonstrate the effectiveness of our method with BLEU scores increment. Discussions of distilled knowledge and further observations of the methods also validate our results.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {mar},
articleno = {42},
numpages = {18},
keywords = {Multilingual neural machine translation, neural machine translation, knowledge distillation, language family}
}

  


@article{arivazhagan2019mnmt,
  author       = {Naveen Arivazhagan and
                  Ankur Bapna and
                  Orhan Firat and
                  Dmitry Lepikhin and
                  Melvin Johnson and
                  Maxim Krikun and
                  Mia Xu Chen and
                  Yuan Cao and
                  George F. Foster and
                  Colin Cherry and
                  Wolfgang Macherey and
                  Zhifeng Chen and
                  Yonghui Wu},
  title        = {Massively Multilingual Neural Machine Translation in the Wild: Findings
                  and Challenges},
  journal      = {CoRR},
  volume       = {abs/1907.05019},
  year         = {2019},
  url          = {http://arxiv.org/abs/1907.05019},
  eprinttype    = {arXiv},
  eprint       = {1907.05019},
  timestamp    = {Thu, 14 Jan 2021 12:12:19 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1907-05019.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{yang2019snapshot,
  title={Snapshot distillation: Teacher-student optimization in one generation},
  author={Yang, Chenglin and Xie, Lingxi and Su, Chi and Yuille, Alan L},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={2859--2868},
  year={2019}
}

@inproceedings{zhang2019your,
  title={Be your own teacher: Improve the performance of convolutional neural networks via self distillation},
  author={Zhang, Linfeng and Song, Jiebo and Gao, Anni and Chen, Jingwei and Bao, Chenglong and Ma, Kaisheng},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={3713--3722},
  year={2019}
}



@inproceedings{diddee-etal-2022-brittle,
    title = "Too Brittle to Touch: Comparing the Stability of Quantization and Distillation towards Developing Low-Resource {MT} Models",
    author = "Diddee, Harshita  and
      Dandapat, Sandipan  and
      Choudhury, Monojit  and
      Ganu, Tanuja  and
      Bali, Kalika",
    editor = {Koehn, Philipp  and
      Barrault, Lo{\"\i}c  and
      Bojar, Ond{\v{r}}ej  and
      Bougares, Fethi  and
      Chatterjee, Rajen  and
      Costa-juss{\`a}, Marta R.  and
      Federmann, Christian  and
      Fishel, Mark  and
      Fraser, Alexander  and
      Freitag, Markus  and
      Graham, Yvette  and
      Grundkiewicz, Roman  and
      Guzman, Paco  and
      Haddow, Barry  and
      Huck, Matthias  and
      Jimeno Yepes, Antonio  and
      Kocmi, Tom  and
      Martins, Andr{\'e}  and
      Morishita, Makoto  and
      Monz, Christof  and
      Nagata, Masaaki  and
      Nakazawa, Toshiaki  and
      Negri, Matteo  and
      N{\'e}v{\'e}ol, Aur{\'e}lie  and
      Neves, Mariana  and
      Popel, Martin  and
      Turchi, Marco  and
      Zampieri, Marcos},
    booktitle = "Proceedings of the Seventh Conference on Machine Translation (WMT)",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates (Hybrid)",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.wmt-1.80",
    pages = "870--885",
    abstract = "Leveraging shared learning through Massively Multilingual Models, state-of-the-art Machine translation (MT) models are often able to adapt to the paucity of data for low-resource languages. However, this performance comes at the cost of significantly bloated models which aren{'}t practically deployable. Knowledge Distillation is one popular technique to develop competitive lightweight models: In this work, we first evaluate its use in compressing MT models, focusing specifically on languages with extremely limited training data. Through our analysis across 8 languages, we find that the variance in the performance of the distilled models due to their dependence on priors including the amount of synthetic data used for distillation, the student architecture, training hyper-parameters and confidence of the teacher models, makes distillation a brittle compression mechanism. To mitigate this, we further explore the use of post-training quantization for the compression of these models. Here, we find that while Distillation provides gains across some low-resource languages, Quantization provides more consistent performance trends for the entire range of languages, especially the lowest-resource languages in our target set.",
}


@article{he2019language,
  title={Language Graph Distillation for Low-Resource Machine Translation},
  author={He, Tianyu and Chen, Jiale and Tan, Xu and Qin, Tao},
  journal={arXiv e-prints},
  pages={arXiv--1908},
  year={2019}
}

@inproceedings{lin2016fixed,
  title={Fixed point quantization of deep convolutional networks},
  author={Lin, Darryl and Talathi, Sachin and Annapureddy, Sreekanth},
  booktitle={International conference on machine learning},
  pages={2849--2858},
  year={2016},
  organization={PMLR}
}


@article{courbariaux2015binaryconnect,
  title={Binaryconnect: Training deep neural networks with binary weights during propagations},
  author={Courbariaux, Matthieu and Bengio, Yoshua and David, Jean-Pierre},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}

@misc{park2024comprehensive,
      title={A Comprehensive Survey of Compression Algorithms for Language Models}, 
      author={Seungcheol Park and Jaehyeon Choi and Sojin Lee and U Kang},
      year={2024},
      eprint={2401.15347},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@inproceedings{song2023letz,
  title={Letz Translate: Low-Resource Machine Translation for Luxembourgish},
  author={Song, Yewei and Ezzini, Saad and Klein, Jacques and Bissyande, Tegawende and Lefebvre, Cl{\'e}ment and Goujon, Anne},
  booktitle={2023 5th International Conference on Natural Language Processing (ICNLP)},
  pages={165--170},
  year={2023},
  organization={IEEE}
}


@InProceedings{zhang2022evaluation,
author="Zhang, Min
and Yang, Hao
and Tao, Shimin
and Zhao, Yanqing
and Qiao, Xiaosong
and Li, Yinlu
and Su, Chang
and Wang, Minghan
and Guo, Jiaxin
and Liu, Yilun
and Qin, Ying",
editor="Sun, Maosong
and Qi, Guilin
and Liu, Kang
and Ren, Jiadong
and Xu, Bin
and Feng, Yansong
and Liu, Yongbin
and Chen, Yubo",
title="Incorporating Multilingual Knowledge Distillation into Machine Translation Evaluation",
booktitle="Knowledge Graph and Semantic Computing: Knowledge Graph Empowers the Digital Economy",
year="2022",
publisher="Springer Nature Singapore",
address="Singapore",
pages="148--160",
abstract="Multilingual knowledge distillation is proposed for multilingual sentence embedding alignment. In this paper, it is found out that multilingual knowledge distillation could implicitly achieve cross-lingual word embedding alignment, which is critically important for reference-free machine translation evaluation (where source texts are directly compared with system translations). Then with the framework of BERTScore, we propose a metric BERTScore-MKD for reference-free machine translation evaluation. From the experimental results on the into-English language pairs of WMT17-19, the reference-free metric BERTScore-MKD is very competitive (not only best mean scores, but also better than BLEU on WMT17-18) when the current state-of-the-art (SOTA) metrics that we know are chosen for comparison. Moreover, the results on WMT19 demonstrate that BERTScore-MKD is also suitable for reference-based machine translation evaluation (where reference texts are used to be compared with system translations).",
isbn="978-981-19-7596-7"
}

@inproceedings{gowda2023cometoid,
  title={Cometoid: Distilling Strong Reference-based Machine Translation Metrics into Even Stronger Quality Estimation Metrics},
  author={Gowda, Thamme and Kocmi, Tom and Junczys-Dowmunt, Marcin},
  booktitle={Proceedings of the Eighth Conference on Machine Translation},
  pages={751--755},
  year={2023}
}

@inproceedings{Yang_2022, series={IJCAI-2022},
   title={UM4: Unified Multilingual Multiple Teacher-Student Model for Zero-Resource Neural Machine Translation},
   url={http://dx.doi.org/10.24963/ijcai.2022/618},
   DOI={10.24963/ijcai.2022/618},
   booktitle={Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence},
   publisher={International Joint Conferences on Artificial Intelligence Organization},
   author={Yang, Jian and Yin, Yuwei and Ma, Shuming and Zhang, Dongdong and Wu, Shuangzhi and Guo, Hongcheng and Li, Zhoujun and Wei, Furu},
   year={2022},
   month=jul, collection={IJCAI-2022} }

@InProceedings{zhang2023Imporance,
author="Zhang, Jiarui
and Huang, Heyan
and Hu, Yue
and Guo, Ping
and Xie, Yuqiang",
editor="Jin, Zhi
and Jiang, Yuncheng
and Buchmann, Robert Andrei
and Bi, Yaxin
and Ghiran, Ana-Maria
and Ma, Wenjun",
title="Importance-Based Neuron Selective Distillation for Interference Mitigation in Multilingual Neural Machine Translation",
booktitle="Knowledge Science, Engineering and Management",
year="2023",
publisher="Springer Nature Switzerland",
address="Cham",
pages="140--150",
abstract="Multilingual neural machine translation employs a single model to translate multiple languages, enabling efficient cross-lingual transferability through shared parameters. However, multilingual training suffers from negative language interference, especially interference with high-resource languages. Existing approaches generally use language-specific modules to distinguish heterogeneous characteristics among different languages but suffer from the parameter explosion problem. In this paper, we propose a ``divide and conquer'' multilingual translation training method based on the importance of neurons that can mitigate negative language interference effectively without adding additional parameters. The key technologies can be summarized as estimation, pruning, distillation, and fine-tuning. Specifically, we estimate the importance of existing pre-trained model neurons, dividing them into the important ones representing general knowledge of each language and the unimportant ones representing individual knowledge of each low-resource language. Then, we prune the pre-trained model, retaining only the important neurons, and train the pruned model supervised by the original complete model via selective distillation to compensate for some performance loss due to unstructured pruning. Finally, we restore the pruned neurons and only fine-tune them. Experimental results on several language pairs demonstrate the effectiveness of the proposed method.",
isbn="978-3-031-40292-0"
}



@misc{lample2018unsupervised,
      title={Unsupervised Machine Translation Using Monolingual Corpora Only}, 
      author={Guillaume Lample and Alexis Conneau and Ludovic Denoyer and Marc'Aurelio Ranzato},
      year={2018},
      eprint={1711.00043},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{johnson2017google,
  title={Google’s multilingual neural machine translation system: Enabling zero-shot translation},
  author={Johnson, Melvin and Schuster, Mike and Le, Quoc V and Krikun, Maxim and Wu, Yonghui and Chen, Zhifeng and Thorat, Nikhil and Vi{\'e}gas, Fernanda and Wattenberg, Martin and Corrado, Greg and others},
  journal={Transactions of the Association for Computational Linguistics},
  volume={5},
  pages={339--351},
  year={2017},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@inproceedings{firat2016zero,
  title={Zero-resource translation with multi-lingual neural machine translation},
  author={Firat, Orhan and Sankaran, Baskaran and Al-Onaizan, Yaser and Vural, Fatos T Yarman and Cho, Kyunghyun},
  booktitle={2016 Conference on Empirical Methods in Natural Language Processing, EMNLP 2016},
  pages={268--277},
  year={2016},
  organization={Association for Computational Linguistics (ACL)}
}

@inproceedings{gordon2020distill,
  title={Distill, Adapt, Distill: Training Small, In-Domain Models for Neural Machine Translation},
  author={Gordon, Mitchell and Duh, Kevin},
  booktitle={Proceedings of the Fourth Workshop on Neural Generation and Translation},
  pages={110--118},
  year={2020}
}

@article{tang2024survey,
  title={A Survey on Transformer Compression},
  author={Tang, Yehui and Wang, Yunhe and Guo, Jianyuan and Tu, Zhijun and Han, Kai and Hu, Hailin and Tao, Dacheng},
  journal={arXiv preprint arXiv:2402.05964},
  year={2024}
}

@article{treviso2023efficient,
  title={Efficient methods for natural language processing: A survey},
  author={Treviso, Marcos and Lee, Ji-Ung and Ji, Tianchu and Aken, Betty van and Cao, Qingqing and Ciosici, Manuel R and Hassid, Michael and Heafield, Kenneth and Hooker, Sara and Raffel, Colin and others},
  journal={Transactions of the Association for Computational Linguistics},
  volume={11},
  pages={826--860},
  year={2023},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…}
}

@ARTICLE{zhang2019future,
  author={Zhang, Biao and Xiong, Deyi and Su, Jinsong and Luo, Jiebo},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing}, 
  title={Future-Aware Knowledge Distillation for Neural Machine Translation}, 
  year={2019},
  volume={27},
  number={12},
  pages={2278-2287},
  keywords={Decoding;History;Context modeling;Predictive models;Computational modeling;Training;Semantics;Future context;knowledge distillation;natural language processing;neural machine translation},
  doi={10.1109/TASLP.2019.2946480}}


@inproceedings{mohammadshahi2022small100,
  title={SMaLL-100: Introducing Shallow Multilingual Machine Translation Model for Low-Resource Languages},
  author={Mohammadshahi, Alireza and Nikoulina, Vassilina and B{\'e}rard, Alexandre and Brun, Caroline and Henderson, James and Besacier, Laurent},
  booktitle={Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  pages={8348--8359},
  year={2022}
}


@article{fan2020englishcentric,
  title={Beyond english-centric multilingual machine translation},
  author={Fan, Angela and Bhosale, Shruti and Schwenk, Holger and Ma, Zhiyi and El-Kishky, Ahmed and Goyal, Siddharth and Baines, Mandeep and Celebi, Onur and Wenzek, Guillaume and Chaudhary, Vishrav and others},
  journal={Journal of Machine Learning Research},
  volume={22},
  number={107},
  pages={1--48},
  year={2021}
}


@article{bapna2022building,
  title={Building machine translation systems for the next thousand languages},
  author={Bapna, Ankur and Caswell, Isaac and Kreutzer, Julia and Firat, Orhan and van Esch, Daan and Siddhant, Aditya and Niu, Mengmeng and Baljekar, Pallavi and Garcia, Xavier and Macherey, Wolfgang and others},
  journal={arXiv preprint arXiv:2205.03983},
  year={2022}
}

@article{li2024mt,
  title={MT-PATCHER: Selective and Extendable Knowledge Distillation from Large Language Models for Machine Translation},
  author={Li, Jiahuan and Cheng, Shanbo and Huang, Shujian and Chen, Jiajun},
  journal={arXiv preprint arXiv:2403.09522},
  year={2024}
}

@article{gordon2019explaining,
  title={Explaining sequence-level knowledge distillation as data-augmentation for neural machine translation},
  author={Gordon, Mitchell A and Duh, Kevin},
  journal={arXiv preprint arXiv:1912.03334},
  year={2019}
}


@article{escolano2022learning,
  title={Learning multilingual and multimodal representations with language-specific encoders and decoders for machine translation},
  author={Escolano Peinado, Carlos},
  year={2022},
  publisher={Universitat Polit{\`e}cnica de Catalunya}
}

@misc{bapna2019simple,
      title={Simple, Scalable Adaptation for Neural Machine Translation}, 
      author={Ankur Bapna and Naveen Arivazhagan and Orhan Firat},
      year={2019},
      eprint={1909.08478},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{koishekenov2023memoryefficient,
      title={Memory-efficient NLLB-200: Language-specific Expert Pruning of a Massively Multilingual Machine Translation Model}, 
      author={Yeskendir Koishekenov and Alexandre Berard and Vassilina Nikoulina},
      year={2023},
      eprint={2212.09811},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{kasai2020deep,
  title={Deep Encoder, Shallow Decoder: Reevaluating Non-autoregressive Machine Translation},
  author={Kasai, Jungo and Pappas, Nikolaos and Peng, Hao and Cross, James and Smith, Noah},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@article{Freitag2017EnsembleDF,
  title={Ensemble Distillation for Neural Machine Translation},
  author={Markus Freitag and Yaser Al-Onaizan and Baskaran Sankaran},
  journal={ArXiv},
  year={2017},
  volume={abs/1702.01802},
  url={https://api.semanticscholar.org/CorpusID:9474415}
}



@inproceedings{fadaee-etal-2017-data,
    title = "Data Augmentation for Low-Resource Neural Machine Translation",
    author = "Fadaee, Marzieh  and
      Bisazza, Arianna  and
      Monz, Christof",
    editor = "Barzilay, Regina  and
      Kan, Min-Yen",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P17-2090",
    doi = "10.18653/v1/P17-2090",
    pages = "567--573",
    abstract = "The quality of a Neural Machine Translation system depends substantially on the availability of sizable parallel corpora. For low-resource language pairs this is not the case, resulting in poor translation quality. Inspired by work in computer vision, we propose a novel data augmentation approach that targets low-frequency words by generating new sentence pairs containing rare words in new, synthetically created contexts. Experimental results on simulated low-resource settings show that our method improves translation quality by up to 2.9 BLEU points over the baseline and up to 3.2 BLEU over back-translation.",
}
@inproceedings{zoph-etal-2016-transfer,
    title = "Transfer Learning for Low-Resource Neural Machine Translation",
    author = "Zoph, Barret  and
      Yuret, Deniz  and
      May, Jonathan  and
      Knight, Kevin",
    editor = "Su, Jian  and
      Duh, Kevin  and
      Carreras, Xavier",
    booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D16-1163",
    doi = "10.18653/v1/D16-1163",
    pages = "1568--1575",
}


@inproceedings{wang-etal-2023-better,
    title = "Better Simultaneous Translation with Monotonic Knowledge Distillation",
    author = "Wang, Shushu  and
      Wu, Jing  and
      Fan, Kai  and
      Luo, Wei  and
      Xiao, Jun  and
      Huang, Zhongqiang",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.131",
    doi = "10.18653/v1/2023.acl-long.131",
    pages = "2334--2349",
    abstract = "Simultaneous machine translation (SiMT) presents a unique challenge as it requires generating target tokens before the source sentence is fully consumed. This can lead to the hallucination problem, where target tokens are generated without support from the source sentence. The prefix-to-prefix training data used to train SiMT models are not always parallel, due to divergent word order between the source and target languages, and can contribute to the problem. In this paper, we propose a novel approach that leverages traditional translation models as teachers and employs a two-stage beam search algorithm to generate monotonic yet accurate reference translations for sequence-level knowledge distillation. Experimental results demonstrate the significant improvements achieved by our approach over multiple strong SiMT baselines, leading to new state-of-the-art performance across various language pairs. Notably, when evaluated on a monotonic version of the WMT15 De-En test set, which includes references generated in a more monotonic style by professional translators, our approach achieves even more substantial improvement over the baselines. The source code and data are publicly available for further exploration.",
}

@misc{li2024mtpatcher,
      title={MT-PATCHER: Selective and Extendable Knowledge Distillation from Large Language Models for Machine Translation}, 
      author={Jiahuan Li and Shanbo Cheng and Shujian Huang and Jiajun Chen},
      year={2024},
      eprint={2403.09522},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@article{Weng_Yu_Huang_Cheng_Luo_2020, title={Acquiring Knowledge from Pre-Trained Model to Neural Machine Translation}, volume={34}, url={https://ojs.aaai.org/index.php/AAAI/article/view/6465}, DOI={10.1609/aaai.v34i05.6465}, abstractNote={&lt;p&gt;Pre-training and fine-tuning have achieved great success in natural language process field. The standard paradigm of exploiting them includes two steps: first, pre-training a model, e.g. BERT, with a large scale unlabeled monolingual data. Then, fine-tuning the pre-trained model with labeled data from downstream tasks. However, in neural machine translation (NMT), we address the problem that the training objective of the bilingual task is far different from the monolingual pre-trained model. This gap leads that only using fine-tuning in NMT can not fully utilize prior language knowledge. In this paper, we propose an A&lt;span style=&quot;font-variant: small-caps;&quot;&gt;pt&lt;/span&gt; framework for acquiring knowledge from pre-trained model to NMT. The proposed approach includes two modules: 1). a dynamic fusion mechanism to fuse task-specific features adapted from general knowledge into NMT network, 2). a knowledge distillation paradigm to learn language knowledge continuously during the NMT training process. The proposed approach could integrate suitable knowledge from pre-trained models to improve the NMT. Experimental results on WMT English to German, German to English and Chinese to English machine translation tasks show that our model outperforms strong baselines and the fine-tuning counterparts.&lt;/p&gt;}, number={05}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Weng, Rongxiang and Yu, Heng and Huang, Shujian and Cheng, Shanbo and Luo, Weihua}, year={2020}, month={Apr.}, pages={9266-9273} }







@inproceedings{miao-etal-2023-exploring,
    title = "Exploring All-In-One Knowledge Distillation Framework for Neural Machine Translation",
    author = "Miao, Zhongjian  and
      Zhang, Wen  and
      Su, Jinsong  and
      Li, Xiang  and
      Luan, Jian  and
      Chen, Yidong  and
      Wang, Bin  and
      Zhang, Min",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.178",
    doi = "10.18653/v1/2023.emnlp-main.178",
    pages = "2929--2940",
    abstract = "Conventional knowledge distillation(KD) approaches are commonly employed to compress neural machine translation(NMT) models. However, they only obtain one lightweight student each time. Consequently, we have to conduct KD multiple times when different students are required at the same time, which could be resource-intensive. Additionally, these students are individually optimized, and thus lack interactions with each other, leading to their potential not being fully exerted. In this work, we propose a novel All-In-One Knowledge Distillation(AIO-KD) framework for NMT, which generates multiple satisfactory students at once. Under AIO-KD, we first randomly extract fewer-layer subnetworks from the teacher as the sample students. Then, we jointly optimize the teacher and these students, where the students simultaneously learn the knowledge from the teacher and interact with other students via mutual learning. When utilized, we re-extract the candidate students, satisfying the specifications of various devices. Particularly, we adopt carefully-designed strategies for AIO-KD: 1) we dynamically detach gradients to prevent poorly-performed students from negatively affecting the teacher during the knowledge transfer, which could subsequently impact other students; 2) we design a two-stage mutual learning strategy, which alleviates the negative impacts of poorly-performed students on the early-stage student interactions. Extensive experiments and in-depth analyses on three benchmarks demonstrate the effectiveness and eco-friendliness of AIO-KD. Our source code is available at https://github.com/DeepLearnXMU/AIO-KD.",
}



@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{liu2022neural,
  title={Neural Machine Translation Transfer Model Based on Mutual Domain Guidance},
  author={Liu, Yupeng and Zhang, Lei and Zhang, Yanan},
  journal={IEEE Access},
  volume={10},
  pages={101595--101608},
  year={2022},
  publisher={IEEE}
}


@inproceedings{zhang-etal-2023-continual,
    title = "Continual Knowledge Distillation for Neural Machine Translation",
    author = "Zhang, Yuanchi  and
      Li, Peng  and
      Sun, Maosong  and
      Liu, Yang",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.443",
    doi = "10.18653/v1/2023.acl-long.443",
    pages = "7978--7996",
    abstract = "While many parallel corpora are not publicly accessible for data copyright, data privacy and competitive differentiation reasons, trained translation models are increasingly available on open platforms. In this work, we propose a method called continual knowledge distillation to take advantage of existing translation models to improve one model of interest. The basic idea is to sequentially transfer knowledge from each trained model to the distilled model. Extensive experiments on Chinese-English and German-English datasets show that our method achieves significant and consistent improvements over strong baselines under both homogeneous and heterogeneous trained model settings and is robust to malicious models.",
}

@incollection{klimaszewski2023gated,
  title={Gated Adapters for Multi-Domain Neural Machine Translation},
  author={Klimaszewski, Mateusz and Belligoli, Zeno and Kumar, Satendra and Stergiadis, Emmanouil},
  booktitle={ECAI 2023},
  pages={1264--1271},
  year={2023},
  publisher={IOS Press}
}



@inproceedings{yang2022making,
  title={Towards making the most of bert in neural machine translation},
  author={Yang, Jiacheng and Wang, Mingxuan and Zhou, Hao and Zhao, Chengqi and Zhang, Weinan and Yu, Yong and Li, Lei},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={34},
  pages={9378--9385},
  year={2020}
}

@article{Goodfellow2013AnEI,
  title={An Empirical Investigation of Catastrophic Forgeting in Gradient-Based Neural Networks},
  author={Ian J. Goodfellow and Mehdi Mirza and Xia Da and Aaron C. Courville and Yoshua Bengio},
  journal={CoRR},
  year={2013},
  volume={abs/1312.6211},
  url={https://api.semanticscholar.org/CorpusID:12730344}
}


@inproceedings{ren-etal-2023-tailoring,
    title = "Tailoring Instructions to Student{'}s Learning Levels Boosts Knowledge Distillation",
    author = "Ren, Yuxin  and
      Zhong, Zihan  and
      Shi, Xingjian  and
      Zhu, Yi  and
      Yuan, Chun  and
      Li, Mu",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.111",
    doi = "10.18653/v1/2023.acl-long.111",
    pages = "1990--2006",
    abstract = "It has been commonly observed that a teacher model with superior performance does not necessarily result in a stronger student, highlighting a discrepancy between current teacher training practices and effective knowledge transfer. In order to enhance the guidance of the teacher training process, we introduce the concept of distillation influence to determine the impact of distillation from each training sample on the student{'}s generalization ability. In this paper, we propose Learning Good Teacher Matters (LGTM), an efficient training technique for incorporating distillation influence into the teacher{'}s learning process. By prioritizing samples that are likely to enhance the student{'}s generalization ability, our LGTM outperforms 10 common knowledge distillation baselines on 6 text classification tasks in the GLUE benchmark.",
}


@inproceedings{stap-monz-2023-multilingual,
    title = "Multilingual $k$-Nearest-Neighbor Machine Translation",
    author = "Stap, David  and
      Monz, Christof",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.571",
    doi = "10.18653/v1/2023.emnlp-main.571",
    pages = "9200--9208",
    abstract = "\textit{k}-nearest-neighbor machine translation has demonstrated remarkable improvements in machine translation quality by creating a datastore of cached examples. However, these improvements have been limited to high-resource language pairs, with large datastores, and remain a challenge for low-resource languages. In this paper, we address this issue by combining representations from multiple languages into a single datastore. Our results consistently demonstrate substantial improvements not only in low-resource translation quality (up to $+3.6$ BLEU), but also for high-resource translation quality (up to $+0.5$ BLEU). Our experiments show that it is possible to create multilingual datastores that are a quarter of the size, achieving a 5.3x speed improvement, by using linguistic similarities for datastore creation.",
}

@ARTICLE{9722996,
  author={Liang, Xiaobo and Wu, Lijun and Li, Juntao and Qin, Tao and Zhang, Min and Liu, Tie-Yan},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing}, 
  title={Multi-Teacher Distillation With Single Model for Neural Machine Translation}, 
  year={2022},
  volume={30},
  number={},
  pages={992-1002},
  keywords={Data models;Transformers;Training;Costs;Task analysis;Machine translation;Decoding;Dropout;knowledge distillation;multiple teachers;neural machine translation;sub-networks},
  doi={10.1109/TASLP.2022.3153264}}


@inproceedings{freitag2023results,
  title={Results of WMT23 metrics shared task: Metrics might be guilty but references are not innocent},
  author={Freitag, Markus and Mathur, Nitika and Lo, Chi-kiu and Avramidis, Eleftherios and Rei, Ricardo and Thompson, Brian and Kocmi, Tom and Blain, Fr{\'e}d{\'e}ric and Deutsch, Daniel and Stewart, Craig and others},
  booktitle={Proceedings of the Eighth Conference on Machine Translation},
  pages={578--628},
  year={2023}
}

@inproceedings{rei2022comet,
  title={COMET-22: Unbabel-IST 2022 submission for the metrics shared task},
  author={Rei, Ricardo and De Souza, Jos{\'e} GC and Alves, Duarte and Zerva, Chrysoula and Farinha, Ana C and Glushkova, Taisiya and Lavie, Alon and Coheur, Luisa and Martins, Andr{\'e} FT},
  booktitle={Proceedings of the Seventh Conference on Machine Translation (WMT)},
  pages={578--585},
  year={2022}
}

@inproceedings{popovic2015chrf,
  title={chrF: character n-gram F-score for automatic MT evaluation},
  author={Popovi{\'c}, Maja},
  booktitle={Proceedings of the tenth workshop on statistical machine translation},
  pages={392--395},
  year={2015}
}

@inproceedings{reimers2020making,
  title={Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation},
  author={Reimers, Nils and Gurevych, Iryna},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={4512--4525},
  year={2020}
}

@inproceedings{zhang2019bertscore,
  title={BERTScore: Evaluating Text Generation with BERT},
  author={Zhang, Tianyi and Kishore, Varsha and Wu, Felix and Weinberger, Kilian Q and Artzi, Yoav},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@inproceedings{rei2022searching,
  title={Searching for COMETINHO: The little metric that could},
  author={Rei, Ricardo and Farinha, Ana C and de Souza, Jos{\'e} GC and Ramos, Pedro G and Martins, Andr{\'e} FT and Coheur, Luisa and Lavie, Alon},
  booktitle={Proceedings of the 23rd Annual Conference of the European Association for Machine Translation},
  pages={61--70},
  year={2022}
}

@inproceedings{papineni2002bleu,
  title={Bleu: a method for automatic evaluation of machine translation},
  author={Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  booktitle={Proceedings of the 40th annual meeting of the Association for Computational Linguistics},
  pages={311--318},
  year={2002}
}

@inproceedings{rei2020comet,
  title={COMET: A Neural Framework for MT Evaluation},
  author={Rei, Ricardo and Stewart, Craig and Farinha, Ana C and Lavie, Alon},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={2685--2702},
  year={2020}
}

@inproceedings{pu2021learning,
  title={Learning Compact Metrics for MT},
  author={Pu, Amy and Chung, Hyung Won and Parikh, Ankur and Gehrmann, Sebastian and Sellam, Thibault},
  booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  pages={751--762},
  year={2021}
}

@inproceedings{sellam2020bleurt,
  title={BLEURT: Learning Robust Metrics for Text Generation},
  author={Sellam, Thibault and Das, Dipanjan and Parikh, Ankur},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  year={2020},
  organization={Association for Computational Linguistics}
}


@inproceedings{mohammadshahi2023compressed,
  title={What Do Compressed Multilingual Machine Translation Models Forget?},
  author={Mohammadshahi, Alireza and Nikoulina, Vassilina and B{\'e}rard, Alexandre and Brun, Caroline and Henderson, James and Besacier, Laurent},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2022},
  pages={4308--4329},
  year={2022}
}

@misc{guerreiro2023hallucinations,
      title={Hallucinations in Large Multilingual Translation Models}, 
      author={Nuno M. Guerreiro and Duarte Alves and Jonas Waldendorf and Barry Haddow and Alexandra Birch and Pierre Colombo and André F. T. Martins},
      year={2023},
      eprint={2303.16104},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{hayashi-etal-2019-findings,
    title = "Findings of the Third Workshop on Neural Generation and Translation",
    author = "Hayashi, Hiroaki  and
      Oda, Yusuke  and
      Birch, Alexandra  and
      Konstas, Ioannis  and
      Finch, Andrew  and
      Luong, Minh-Thang  and
      Neubig, Graham  and
      Sudoh, Katsuhito",
    editor = "Birch, Alexandra  and
      Finch, Andrew  and
      Hayashi, Hiroaki  and
      Konstas, Ioannis  and
      Luong, Thang  and
      Neubig, Graham  and
      Oda, Yusuke  and
      Sudoh, Katsuhito",
    booktitle = "Proceedings of the 3rd Workshop on Neural Generation and Translation",
    month = nov,
    year = "2019",
    address = "Hong Kong",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-5601",
    doi = "10.18653/v1/D19-5601",
    pages = "1--14",
    abstract = "This document describes the findings of the Third Workshop on Neural Generation and Translation, held in concert with the annual conference of the Empirical Methods in Natural Language Processing (EMNLP 2019). First, we summarize the research trends of papers presented in the proceedings. Second, we describe the results of the two shared tasks 1) efficient neural machine translation (NMT) where participants were tasked with creating NMT systems that are both accurate and efficient, and 2) document generation and translation (DGT) where participants were tasked with developing systems that generate summaries from structured data, potentially with assistance from text in another language.",
}

@inproceedings{Li2020LearningLT,
  title={Learning Light-Weight Translation Models from Deep Transformer},
  author={Bei Li and Ziyang Wang and Hui Liu and Quan Du and Tong Xiao and Chunliang Zhang and Jingbo Zhu},
  booktitle={AAAI Conference on Artificial Intelligence},
  year={2020},
  url={https://api.semanticscholar.org/CorpusID:229678023}
}

@inproceedings{setiawan-2024-accurate,
    title = "Accurate Knowledge Distillation via n-best Reranking",
    author = "Setiawan, Hendra",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.naacl-long.72",
    doi = "10.18653/v1/2024.naacl-long.72",
    pages = "1330--1345",
    abstract = "We propose utilizing n-best reranking to enhance Sequence-Level Knowledge Distillation (Kim and Rush, 2016) where we extract pseudo-labels for student model{'}s training data from top n-best hypotheses and leverage a diverse set of models with different inductive biases, objective functions or architectures, including some publicly-available large language models, to pick the highest-quality hypotheses as labels. The effectiveness of our proposal is validated through experiments on the WMT{'}21 German ↔ English and Chinese ↔ English translation tasks. Our results demonstrate that utilizing pseudo-labels generated by our n-best reranker leads to a significantly more accurate student model. In fact, our best student model achieves comparable accuracy to a large translation model from (Tran et al., 2021) with 4.7 billion parameters, while having two orders of magnitude fewer parameters.",
}

@inproceedings{
agarwal2024onpolicy,
title={On-Policy Distillation of Language Models: Learning from Self-Generated Mistakes},
author={Rishabh Agarwal and Nino Vieillard and Yongchao Zhou and Piotr Stanczyk and Sabela Ramos Garea and Matthieu Geist and Olivier Bachem},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=3zKtaqxLhW}
}
@inproceedings{wen-etal-2023-f,
    title = "f-Divergence Minimization for Sequence-Level Knowledge Distillation",
    author = "Wen, Yuqiao  and
      Li, Zichao  and
      Du, Wenyu  and
      Mou, Lili",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.605",
    doi = "10.18653/v1/2023.acl-long.605",
    pages = "10817--10834",
    abstract = "Knowledge distillation (KD) is the process of transferring knowledge from a large model to a small one. It has gained increasing attention in the natural language processing community, driven by the demands of compressing ever-growing language models. In this work, we propose an FDISTILL framework, which formulates sequence-level knowledge distillation as minimizing a generalized f-divergence function. We propose four distilling variants under our framework and show that existing SeqKD and ENGINE approaches are approximations of our FDISTILL methods. We further derive step-wise decomposition for our FDISTILL, reducing intractable sequence-level divergence to word-level losses that can be computed in a tractable manner. Experiments across four datasets show that our methods outperform existing KD approaches, and that our symmetric distilling losses can better force the student to learn from the teacher distribution.",
}
@inproceedings{deng2022improving,
  title={Improving simultaneous machine translation with monolingual data},
  author={Deng, Hexuan and Ding, Liang and Liu, Xuebo and Zhang, Meishan and Tao, Dacheng and Zhang, Min},
  booktitle={Proceedings of the Thirty-Seventh AAAI Conference on Artificial Intelligence and Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence and Thirteenth Symposium on Educational Advances in Artificial Intelligence},
  pages={12728--12736},
  year={2023}
}

}

@inproceedings{yoon2023emnetwork,
  title={EM-network: oracle guided self-distillation for sequence learning},
  author={Yoon, Ji Won and Ahn, Sunghwan and Lee, Hyeonseung and Kim, Minchan and Kim, Seok Min and Kim, Nam Soo},
  booktitle={International Conference on Machine Learning},
  pages={40111--40128},
  year={2023},
  organization={PMLR}
}

@article{WAN2024101583,
title = {Dual Knowledge Distillation for neural machine translation},
journal = {Computer Speech \& Language},
volume = {84},
pages = {101583},
year = {2024},
issn = {0885-2308},
doi = {https://doi.org/10.1016/j.csl.2023.101583},
url = {https://www.sciencedirect.com/science/article/pii/S088523082300102X},
author = {Yuxian Wan and Wenlin Zhang and Zhen Li and Hao Zhang and Yanxia Li},
keywords = {Knowledge distillation, k Nearest Neighbor Knowledge Distillation, Low-resource, Monolingual data},
abstract = {Existing knowledge distillation methods use large amount of bilingual data and focus on mining the corresponding knowledge distribution between the source language and the target language. However, for some languages, bilingual data is not abundant. In this paper, to make better use of both monolingual and limited bilingual data, we propose a new knowledge distillation method called Dual Knowledge Distillation (DKD). For monolingual data, we use a self-distillation strategy which combines self-training and knowledge distillation for the encoder to extract more consistent monolingual representation. For bilingual data, on top of the k Nearest Neighbor Knowledge Distillation (kNN-KD) method, a similar self-distillation strategy is adopted as a consistency regularization method to force the decoder to produce consistent output. Experiments on standard datasets, multi-domain translation datasets, and low-resource datasets show that DKD achieves consistent improvements over state-of-the-art baselines including kNN-KD.}
}

@inproceedings{sen-etal-2023-self,
    title = "Self-training Reduces Flicker in Retranslation-based Simultaneous Translation",
    author = "Sen, Sukanta  and
      Sennrich, Rico  and
      Zhang, Biao  and
      Haddow, Barry",
    editor = "Vlachos, Andreas  and
      Augenstein, Isabelle",
    booktitle = "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics",
    month = may,
    year = "2023",
    address = "Dubrovnik, Croatia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.eacl-main.270",
    doi = "10.18653/v1/2023.eacl-main.270",
    pages = "3734--3744",
    abstract = "In simultaneous translation, the retranslation approach has the advantage of requiring no modifications to the inference engine. However, in order to reduce the undesirable flicker in the output, previous work has resorted to increasing the latency through masking, and introducing specialised inference, thus losing the simplicity of the approach. In this work, we show that self-training improves the flicker-latency tradeoff, while maintaining similar translation quality to the original. Our analysis indicates that self-training reduces flicker by controlling monotonicity. Furthermore, self-training can be combined with biased beam search to further improve the flicker-latency tradeoff.",
}

%  article{ HyojinJeon2023PETPK,
%  title={PET: Parameter-efficient Knowledge Distillation on Transformer},
%  author={ID HyojinJeon and Seungcheol Park and Jin-Gee Kim and ID U.Kang},
%  journal={PLOS ONE},
%  year={2023},
%  volume={18},
%  url={https://api.semanticscholar.org/CorpusID:259357156}
% }

@article{HyojinJeon2023PETPK,
    doi = {10.1371/journal.pone.0288060},
    author = {Jeon, Hyojin AND Park, Seungcheol AND Kim, Jin-Gee AND Kang, U.},
    journal = {PLOS ONE},
    publisher = {Public Library of Science},
    title = {PET: Parameter-efficient Knowledge Distillation on Transformer},
    year = {2023},
    month = {07},
    volume = {18},
    url = {https://doi.org/10.1371/journal.pone.0288060},
    pages = {1-21},
    abstract = {Given a large Transformer model, how can we obtain a small and computationally efficient model which maintains the performance of the original model? Transformer has shown significant performance improvements for many NLP tasks in recent years. However, their large size, expensive computational cost, and long inference time make it challenging to deploy them to resource-constrained devices. Existing Transformer compression methods mainly focus on reducing the size of the encoder ignoring the fact that the decoder takes the major portion of the long inference time. In this paper, we propose PET (Parameter-Efficient knowledge distillation on Transformer), an efficient Transformer compression method that reduces the size of both the encoder and decoder. In PET, we identify and exploit pairs of parameter groups for efficient weight sharing, and employ a warm-up process using a simplified task to increase the gain through Knowledge Distillation. Extensive experiments on five real-world datasets show that PET outperforms existing methods in machine translation tasks. Specifically, on the IWSLT’14 EN→DE task, PET reduces the memory usage by 81.20% and accelerates the inference speed by 45.15% compared to the uncompressed model, with a minor decrease in BLEU score of 0.27.},
    number = {7},

}


@inproceedings{zhuang-tu-2023-pretrained,
    title = "Pretrained Bidirectional Distillation for Machine Translation",
    author = "Zhuang, Yimeng  and
      Tu, Mei",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.63",
    doi = "10.18653/v1/2023.acl-long.63",
    pages = "1132--1145",
    abstract = "Knowledge transfer can boost neural machine translation (NMT), for example, by finetuning a pretrained masked language model (LM). However, it may suffer from the forgetting problem and the structural inconsistency between pretrained LMs and NMT models. Knowledge distillation (KD) may be a potential solution to alleviate these issues, but few studies have investigated language knowledge transfer from pretrained language models to NMT models through KD. In this paper, we propose Pretrained Bidirectional Distillation (PBD) for NMT, which aims to efficiently transfer bidirectional language knowledge from masked language pretraining to NMT models. Its advantages are reflected in efficiency and effectiveness through a globally defined and bidirectional context-aware distillation objective. Bidirectional language knowledge of the entire sequence is transferred to an NMT model concurrently during translation training. Specifically, we propose self-distilled masked language pretraining to obtain the PBD objective. We also design PBD losses to efficiently distill the language knowledge, in the form of token probabilities, to the encoder and decoder of an NMT model using the PBD objective. Extensive experiments reveal that pretrained bidirectional distillation can significantly improve machine translation performance and achieve competitive or even better results than previous pretrain-finetune or unified multilingual translation methods in supervised, unsupervised, and zero-shot scenarios. Empirically, it is concluded that pretrained bidirectional distillation is an effective and efficient method for transferring language knowledge from pretrained language models to NMT models.",
}


@inproceedings{zhang-etal-2023-towards-understanding,
    title = "Towards Understanding and Improving Knowledge Distillation for Neural Machine Translation",
    author = "Zhang, Songming  and
      Liang, Yunlong  and
      Wang, Shuaibo  and
      Chen, Yufeng  and
      Han, Wenjuan  and
      Liu, Jian  and
      Xu, Jinan",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.448",
    doi = "10.18653/v1/2023.acl-long.448",
    pages = "8062--8079",
    abstract = "Knowledge distillation (KD) is a promising technique for model compression in neural machine translation. However, where the knowledge hides in KD is still not clear, which may hinder the development of KD. In this work, we first unravel this mystery from an empirical perspective and show that the knowledge comes from the top-1 predictions of teachers, which also helps us build a potential connection between word- and sequence-level KD. Further, we point out two inherent issues in vanilla word-level KD based on this finding. Firstly, the current objective of KD spreads its focus to whole distributions to learn the knowledge, yet lacks special treatment on the most crucial top-1 information. Secondly, the knowledge is largely covered by the golden information due to the fact that most top-1 predictions of teachers overlap with ground-truth tokens, which further restricts the potential of KD. To address these issues, we propose a new method named Top-1 Information Enhanced Knowledge Distillation (TIE-KD). Specifically, we design a hierarchical ranking loss to enforce the learning of the top-1 information from the teacher. Additionally, we develop an iterative KD procedure to infuse more additional knowledge by distilling on the data without ground-truth targets. Experiments on WMT{'}14 English-German, WMT{'}14 English-French and WMT{'}16 English-Romanian demonstrate that our method can respectively boost Transformer$_{base}$ students by +1.04, +0.60 and +1.11 BLEU scores and significantly outperforms the vanilla word-level KD baseline. Besides, our method shows higher generalizability on different teacher-student capacity gaps than existing KD techniques.",
}

@article{wei2024sentencelevel,
  title={Sentence-Level or Token-Level? A Comprehensive Study on Knowledge Distillation},
  author={Wei, Jingxuan and Sun, Linzhuang and Leng, Yichong and Tan, Xu and Yu, Bihui and Guo, Ruifeng},
  journal={arXiv preprint arXiv:2404.14827},
  year={2024}
}


@inproceedings{jin2024align,
  title={Align-to-Distill: Trainable Attention Alignment for Knowledge Distillation in Neural Machine Translation},
  author={Jin, Heegon and Son, Seonil and Park, Jemin and Kim, Youngseok and Noh, Hyungjong and Lee, Yeonsoo},
  booktitle={Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)},
  pages={722--732},
  year={2024}
}


@misc{khandelwal2021nearest,
      title={Nearest Neighbor Machine Translation}, 
      author={Urvashi Khandelwal and Angela Fan and Dan Jurafsky and Luke Zettlemoyer and Mike Lewis},
      year={2021},
      eprint={2010.00710},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{mirzadeh2020improved,
  title={Improved knowledge distillation via teacher assistant},
  author={Mirzadeh, Seyed Iman and Farajtabar, Mehrdad and Li, Ang and Levine, Nir and Matsukawa, Akihiro and Ghasemzadeh, Hassan},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={34},
  pages={5191--5198},
  year={2020}
}

junczys-dowmunt-etal-2018-marian-costj@inproceedings{chen2014systematic,
  title={A systematic comparison of smoothing techniques for sentence-level BLEU},
  author={Chen, Boxing and Cherry, Colin},
  booktitle={Proceedings of the ninth workshop on statistical machine translation},
  pages={362--367},
  year={2014}
}

@article{gupta2021compression,
  title={Compression of deep learning models for text: A survey},
  author={Gupta, Manish and Agrawal, Puneet},
  journal={ACM Transactions on Knowledge Discovery from Data (TKDD)},
  volume={16},
  number={4},
  pages={1--55},
  year={2022},
  publisher={ACM New York, NY}
}

@inproceedings{dabre2017empirical,
  title={An empirical study of language relatedness for transfer learning in neural machine translation},
  author={Dabre, Raj and Nakagawa, Tetsuji and Kazawa, Hideto},
  booktitle={Proceedings of the 31st Pacific Asia conference on language, information and computation},
  pages={282--286},
  year={2017}
}

@article{ustun2024aya,
  title={Aya model: An instruction finetuned open-access multilingual language model},
  author={{\"U}st{\"u}n, Ahmet and Aryabumi, Viraat and Yong, Zheng-Xin and Ko, Wei-Yin and D'souza, Daniel and Onilude, Gbemileke and Bhandari, Neel and Singh, Shivalika and Ooi, Hui-Lee and Kayid, Amr and others},
  journal={arXiv preprint arXiv:2402.07827},
  year={2024}
}

@inproceedings{conneau2020unsupervised,
  title={Unsupervised Cross-lingual Representation Learning at Scale},
  author={Conneau, Alexis and Khandelwal, Kartikay and Goyal, Naman and Chaudhary, Vishrav and Wenzek, Guillaume and Guzm{\'a}n, Francisco and Grave, {\'E}douard and Ott, Myle and Zettlemoyer, Luke and Stoyanov, Veselin},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={8440--8451},
  year={2020}
}

@inproceedings{zhang2021bridging,
  author={Zhang, Wen and Feng, Yang and Liu, Qun},
  booktitle={Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence},
  pages={4790--4794},
  year={2021}
}
@inproceedings{rei2021references,
  title={Are references really needed? unbabel-IST 2021 submission for the metrics shared task},
  author={Rei, Ricardo and Farinha, Ana C and Zerva, Chrysoula and van Stigt, Daan and Stewart, Craig and Ramos, Pedro and Glushkova, Taisiya and Martins, Andr{\'e} FT and Lavie, Alon},
  booktitle={Proceedings of the Sixth Conference on Machine Translation},
  pages={1030--1040},
  year={2021}
}
@inproceedings{lee2022hard,
  title={Hard Gate Knowledge Distillation-Leverage Calibration for Robust and Reliable Language Model},
  author={Lee, Dongkyu and Tian, Zhiliang and Zhao, Yingxiu and Cheung, Ka Chun and Zhang, Nevin},
  booktitle={Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  pages={9793--9803},
  year={2022}
}

@inproceedings{roy-etal-2024-enhancing,
    title = "Enhancing Low-Resource {NMT} with a Multilingual Encoder and Knowledge Distillation: A Case Study",
    author = "Roy, Aniruddha  and
      Ray, Pretam  and
      Maheshwari, Ayush  and
      Sarkar, Sudeshna  and
      Goyal, Pawan",
    editor = "Ojha, Atul Kr.  and
      Liu, Chao-hong  and
      Vylomova, Ekaterina  and
      Pirinen, Flammie  and
      Abbott, Jade  and
      Washington, Jonathan  and
      Oco, Nathaniel  and
      Malykh, Valentin  and
      Logacheva, Varvara  and
      Zhao, Xiaobing",
    booktitle = "Proceedings of the The Seventh Workshop on Technologies for Machine Translation of Low-Resource Languages (LoResMT 2024)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.loresmt-1.7",
    pages = "64--73",
    abstract = "Neural Machine Translation (NMT) remains a formidable challenge, especially when dealing with low-resource languages. Pre-trained sequence-to-sequence (seq2seq) multi-lingual models, such as mBART-50, have demonstrated impressive performance in various low-resource NMT tasks. However, their pre-training has been confined to 50 languages, leaving out support for numerous low-resource languages, particularly those spoken in the Indian subcontinent. Expanding mBART-50{'}s language support requires complex pre-training, risking performance decline due to catastrophic forgetting. Considering these expanding challenges, this paper explores a framework that leverages the benefits of a pre-trained language model along with knowledge distillation in a seq2seq architecture to facilitate translation for low-resource languages, including those not covered by mBART-50. The proposed framework employs a multilingual encoder-based seq2seq model as the foundational architecture and subsequently uses complementary knowledge distillation techniques to mitigate the impact of imbalanced training. Our framework is evaluated on three low-resource Indic languages in four Indic-to-Indic directions, yielding significant BLEU-4 and chrF improvements over baselines. Further, we conduct human evaluation to confirm effectiveness of our approach. Our code is publicly available at https://github.com/raypretam/Two-step-low-res-NMT.",
}
@inproceedings{gu2018non,
  title={Non-Autoregressive Neural Machine Translation},
  author={Gu, Jiatao and Bradbury, James and Xiong, Caiming and Li, Victor OK and Socher, Richard},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@article{galaindictrans2,
  title={IndicTrans2: Towards High-Quality and Accessible Machine Translation Models for all 22 Scheduled Indian Languages},
  author={Gala, Jay and Chitale, Pranjal A and Raghavan, AK and Gumma, Varun and Doddapaneni, Sumanth and Nawale, Janki Atul and Sujatha, Anupama and Puduppully, Ratish and Raghavan, Vivek and Kumar, Pratyush and others},
  year=2023,
  journal={Transactions on Machine Learning Research}
}

@inproceedings{ahmed2024neural,
  title={Neural Machine Translation between Low-Resource Languages with Synthetic Pivoting},
  author={Ahmed, Khalid and Buys, Jan},
  booktitle={Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)},
  pages={12144--12158},
  year={2024}
}

@article{enis2024llm,
  title={From LLM to NMT: Advancing Low-Resource Machine Translation with Claude},
  author={Enis, Maxim and Hopkins, Mark},
  journal={arXiv preprint arXiv:2404.13813},
  year={2024}
}


@inproceedings{van2017dynamic,
  title={Dynamic Data Selection for Neural Machine Translation},
  author={van der Wees, Marlies and Bisazza, Arianna and Monz, Christof},
  booktitle={Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
  pages={1400--1410},
  year={2017}
}

@article{zhang2024distilling,
  title={Distilling BERT knowledge into Seq2Seq with regularized Mixup for low-resource neural machine translation},
  author={Zhang, Guanghua and Liu, Hua and Guo, Junjun and Guo, Tianyu},
  journal={Expert Systems with Applications},
  pages={125314},
  year={2024},
  publisher={Elsevier}
}

@inproceedings{gu2018non2,
  title={Non-Autoregressive Neural Machine Translation},
  author={Gu, Jiatao and Bradbury, James and Xiong, Caiming and Li, Victor OK and Socher, Richard},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@inproceedings{zhou2020improving,
  title={Improving Non-autoregressive Neural Machine Translation with Monolingual Data},
  author={Zhou, Jiawei and Keung, Phillip},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={1893--1898},
  year={2020}
}


@inproceedings{zhouunderstanding,
  title={Understanding Knowledge Distillation in Non-autoregressive Machine Translation},
  author={Zhou, Chunting and Gu, Jiatao and Neubig, Graham},
  booktitle={International Conference on Learning Representations},
  year="2020"
}

@inproceedings{shen2019mixture,
  title={Mixture models for diverse machine translation: Tricks of the trade},
  author={Shen, Tianxiao and Ott, Myle and Auli, Michael and Ranzato, Marc’Aurelio},
  booktitle={International conference on machine learning},
  pages={5719--5728},
  year={2019},
  organization={PMLR}
}

@inproceedings{furlanello2018born,
  title={Born again neural networks},
  author={Furlanello, Tommaso and Lipton, Zachary and Tschannen, Michael and Itti, Laurent and Anandkumar, Anima},
  booktitle={International conference on machine learning},
  pages={1607--1616},
  year={2018},
  organization={PMLR}
}

@inproceedings{shao2022one,
  title={One Reference Is Not Enough: Diverse Distillation with Reference Selection for Non-Autoregressive Translation},
  author={Shao, Chenze and Wu, Xuanfu and Feng, Yang},
  booktitle={Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={3779--3791},
  year={2022}
}

@article{guo2021self,
  title={Self-distillation mixup training for non-autoregressive neural machine translation},
  author={Guo, Jiaxin and Wang, Minghan and Wei, Daimeng and Shang, Hengchao and Wang, Yuxia and Li, Zongyao and Yu, Zhengzhe and Wu, Zhanglin and Chen, Yimeng and Su, Chang and others},
  journal={arXiv preprint arXiv:2112.11640},
  year={2021}
}

@inproceedings{liu2023selective,
  title={Selective knowledge distillation for non-autoregressive neural machine translation},
  author={Liu, Min and Bao, Yu and Zhao, Chengqi and Huang, Shujian},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={37},
  number={11},
  pages={13246--13254},
  year={2023}
}

@inproceedings{xu2021does,
  title={How Does Distilled Data Complexity Impact the Quality and Confidence of Non-Autoregressive Machine Translation?},
  author={Xu, Weijia and Ma, Shuming and Zhang, Dongdong and Carpuat, Marine},
  booktitle={Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021},
  pages={4392--4400},
  year={2021}
}

@inproceedings{ding2021understanding,
title={Understanding and Improving Lexical Choice in Non-Autoregressive Translation},
author={Liang Ding and Longyue Wang and Xuebo Liu and Derek F. Wong and Dacheng Tao and Zhaopeng Tu},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=ZTFeSBIX9C}
}

@inproceedings{ghazvininejad2019mask,
  title={Mask-Predict: Parallel Decoding of Conditional Masked Language Models},
  author={Ghazvininejad, Marjan and Levy, Omer and Liu, Yinhan and Zettlemoyer, Luke},
  booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  pages={6112--6121},
  year={2019}
}

@article{gu2019levenshtein,
  title={Levenshtein transformer},
  author={Gu, Jiatao and Wang, Changhan and Zhao, Junbo},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{xiao2023survey,
  title={A survey on non-autoregressive generation for neural machine translation and beyond},
  author={Xiao, Yisheng and Wu, Lijun and Guo, Junliang and Li, Juntao and Zhang, Min and Qin, Tao and Liu, Tie-yan},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume={45},
  number={10},
  pages={11407--11427},
  year={2023},
  publisher={IEEE}
}


@inproceedings{ding2021rejuvenating,
  title={Rejuvenating Low-Frequency Words: Making the Most of Parallel Data in Non-Autoregressive Translation},
  author={Ding, Liang and Wang, Longyue and Liu, Xuebo and Wong, Derek F and Tao, Dacheng and Tu, Zhaopeng},
  booktitle={Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  pages={3431--3441},
  year={2021}
}

@misc{wang2024dontthrowawaydata,
      title={Don't Throw Away Data: Better Sequence Knowledge Distillation}, 
      author={Jun Wang and Eleftheria Briakou and Hamid Dadkhahi and Rishabh Agarwal and Colin Cherry and Trevor Cohn},
      year={2024},
      eprint={2407.10456},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.10456}, 
}
@misc{finkelstein2024mbrqefinetuningtrainingtime,
      title={MBR and QE Finetuning: Training-time Distillation of the Best and Most Expensive Decoding Methods}, 
      author={Mara Finkelstein and Subhajit Naskar and Mehdi Mirzazadeh and Apurva Shah and Markus Freitag},
      year={2024},
      eprint={2309.10966},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2309.10966}, 
}

@inproceedings{lv-etal-2024-taekd,
    title = "{TA}e{KD}: Teacher Assistant Enhanced Knowledge Distillation for Closed-Source Multilingual Neural Machine Translation",
    author = "Lv, Bo  and
      Liu, Xin  and
      Wei, Kaiwen  and
      Luo, Ping  and
      Yu, Yue",
    editor = "Calzolari, Nicoletta  and
      Kan, Min-Yen  and
      Hoste, Veronique  and
      Lenci, Alessandro  and
      Sakti, Sakriani  and
      Xue, Nianwen",
    booktitle = "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)",
    month = may,
    year = "2024",
    address = "Torino, Italia",
    publisher = "ELRA and ICCL",
    url = "https://aclanthology.org/2024.lrec-main.1350",
    pages = "15530--15541",
    }

@misc{enis2024llmnmtadvancinglowresource,
      title={From LLM to NMT: Advancing Low-Resource Machine Translation with Claude}, 
      author={Maxim Enis and Mark Hopkins},
      year={2024},
      eprint={2404.13813},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2404.13813}, 
}

@inproceedings{zhong-etal-2024-revisiting,
    title = "Revisiting Knowledge Distillation for Autoregressive Language Models",
    author = "Zhong, Qihuang  and
      Ding, Liang  and
      Shen, Li  and
      Liu, Juhua  and
      Du, Bo  and
      Tao, Dacheng",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.587",
    doi = "10.18653/v1/2024.acl-long.587",
    pages = "10900--10913",
    abstract = "Knowledge distillation (KD) is a common approach to compress a teacher model to reduce its inference cost and memory footprint, by training a smaller student model. However, in the context of autoregressive language models (LMs), we empirically find that larger teacher LMs might dramatically result in a poorer student. In response to this problem, we conduct a series of analyses and reveal that different tokens have different teaching modes, neglecting which will lead to performance degradation. Motivated by this, we propose a simple yet effective adaptive teaching approach (ATKD) to improve the KD. The core of ATKD is to reduce rote learning and make teaching more diverse and flexible. Extensive experiments on 8 LM tasks show that, with the help of ATKD, various baseline KD methods can achieve consistent and significant performance gains (up to +3.04{\%} average score) across all model types and sizes. More encouragingly, ATKD can improve the student model generalization effectively.",
}

@misc{aji2021fullysyntheticdataimproves,
      title={Fully Synthetic Data Improves Neural Machine Translation with Knowledge Distillation}, 
      author={Alham Fikri Aji and Kenneth Heafield},
      year={2021},
      eprint={2012.15455},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2012.15455}, 
}
@software{benallal2024cosmopedia,
  author = {Ben Allal, Loubna and Lozhkov, Anton and Penedo, Guilherme and Wolf, Thomas and von Werra, Leandro},
  title = {Cosmopedia},
  month = February,
  year = 2024,
  url = {https://huggingface.co/datasets/HuggingFaceTB/cosmopedia}
}

@InProceedings{pmlr-v15-ross11a,
  title = 	 {A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning},
  author = 	 {Ross, Stephane and Gordon, Geoffrey and Bagnell, Drew},
  booktitle = 	 {Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {627--635},
  year = 	 {2011},
  editor = 	 {Gordon, Geoffrey and Dunson, David and Dudík, Miroslav},
  volume = 	 {15},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Fort Lauderdale, FL, USA},
  month = 	 {11--13 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v15/ross11a/ross11a.pdf},
  url = 	 {https://proceedings.mlr.press/v15/ross11a.html},
  abstract = 	 {Sequential prediction problems such as imitation learning, where future observations depend on previous predictions (actions), violate the common i.i.d. assumptions made in statistical learning. This leads to poor performance in theory and often in practice. Some recent approaches provide stronger guarantees in this setting, but remain somewhat unsatisfactory as they train either non-stationary or stochastic policies and require a large number of iterations. In this paper, we propose a new iterative algorithm, which trains a stationary deterministic policy, that can be seen as a no regret algorithm in an online learning setting. We show that any such no regret algorithm, combined with additional reduction assumptions, must find a policy with good performance under the distribution of observations it induces in such sequential settings. We demonstrate that this new approach outperforms previous approaches on two challenging imitation learning problems and a benchmark sequence labeling problem.}
}

@article{yang2018knowledge,
  title={Knowledge distillation in generations: More tolerant teachers educate better students},
  author={Yang, Chenglin and Xie, Lingxi and Qiao, Siyuan and Yuille, Alan},
  journal={arXiv preprint arXiv:1805.05551},
  year={2018}
}
